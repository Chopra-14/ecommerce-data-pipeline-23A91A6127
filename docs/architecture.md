# ğŸ—ï¸ System Architecture â€” E-Commerce Data Pipeline

## ğŸ“Œ Overview
This project implements a **modular, end-to-end ETL data pipeline** for an E-Commerce analytics platform.  
The architecture is designed to be **scalable, idempotent, fault-tolerant, and analytics-ready**, supporting batch processing, monitoring, and BI reporting.

---

## ğŸ”¹ High-Level Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Generation â”‚
â”‚ (Synthetic CSVs) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Staging Layer â”‚
â”‚ (Raw Ingestion) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Quality Layer â”‚
â”‚ (Validation & DQ) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Production Layer â”‚
â”‚ (Cleaned Tables) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Warehouse Layer â”‚
â”‚ (Star Schema) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Analytics Layer â”‚
â”‚ (Aggregations) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BI / Dashboards â”‚
â”‚ (Power BI) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

yaml
Copy code

---

## ğŸ§± Architecture Layers Explained

### 1ï¸âƒ£ Data Generation Layer
**Purpose:** Create reproducible synthetic e-commerce data.

- Implemented using Python + Faker
- Generates:
  - Customers
  - Products
  - Transactions
  - Transaction Items
- Output format: CSV
- Idempotent (files overwritten on each run)

ğŸ“ Location:
scripts/data_generation/
data/raw/

yaml
Copy code

---

### 2ï¸âƒ£ Staging Layer
**Purpose:** Raw ingestion without business logic.

- Loads CSVs into PostgreSQL `staging` schema
- Uses `TRUNCATE + INSERT` for idempotency
- Adds metadata columns like `loaded_at`

ğŸ“ Location:
scripts/ingestion/
PostgreSQL schema: staging

yaml
Copy code

---

### 3ï¸âƒ£ Data Quality Layer
**Purpose:** Validate correctness and reliability of data.

Checks performed:
- Referential integrity (orphan records)
- NULL checks on mandatory columns
- Basic consistency rules
- Quality score calculation

Outputs:
- `data_quality_report.json`

ğŸ“ Location:
scripts/quality_checks/
data/processed/data_quality_report.json

yaml
Copy code

---

### 4ï¸âƒ£ Production Layer
**Purpose:** Cleaned and business-ready transactional data.

Key transformations:
- Data cleansing (trim, lowercase emails)
- Business logic application
- Constraint enforcement
- Deduplication

Idempotent behavior:
- Dimensions truncated and reloaded
- Facts inserted safely

ğŸ“ Location:
scripts/transformation/staging_to_production.py
PostgreSQL schema: production

markdown
Copy code

---

### 5ï¸âƒ£ Warehouse Layer (Dimensional Model)
**Purpose:** Analytics-optimized star schema.

Schema design:
- **Fact Table**
  - `fact_sales`
- **Dimension Tables**
  - `dim_customers`
  - `dim_products`
  - `dim_date`
  - `dim_payment_method`
- **Aggregate Tables**
  - `agg_daily_sales`
  - `agg_customer_metrics`
  - `agg_product_performance`

Features:
- Surrogate keys
- SCD handling
- Optimized for BI tools

ğŸ“ Location:
scripts/transformation/load_warehouse.py
PostgreSQL schema: warehouse

yaml
Copy code

---

### 6ï¸âƒ£ Analytics Layer
**Purpose:** Pre-computed analytics for dashboards.

- Executes SQL aggregations
- Exports CSV outputs
- Used by Power BI dashboards

ğŸ“ Location:
scripts/transformation/generate_analytics.py
data/processed/analytics/

yaml
Copy code

---

### 7ï¸âƒ£ Orchestration Layer
**Purpose:** Control execution order and fault handling.

- Executes pipeline steps sequentially
- Stops execution on failure
- Retry logic with exponential backoff
- Generates execution report

ğŸ“ Location:
scripts/pipeline_orchestrator.py
data/processed/pipeline_execution_report.json

yaml
Copy code

---

### 8ï¸âƒ£ Scheduling & Automation
**Purpose:** Fully automated daily pipeline execution.

- Uses Python scheduler
- Prevents concurrent runs
- Executes cleanup after success

ğŸ“ Location:
scripts/scheduler.py
scripts/cleanup_old_data.py
logs/scheduler_activity.log

yaml
Copy code

---

### 9ï¸âƒ£ Monitoring & Alerting
**Purpose:** Continuous pipeline health monitoring.

Monitored dimensions:
- Pipeline execution health
- Data freshness
- Volume anomalies
- Data quality trends
- Database connectivity

Output:
- `monitoring_report.json`

ğŸ“ Location:
scripts/monitoring/
data/processed/monitoring_report.json

yaml
Copy code

---

### ğŸ”Ÿ Business Intelligence Layer
**Purpose:** Data visualization and insights.

- Tool: **Power BI**
- Dashboards:
  - Executive Overview
  - Product Performance
  - Customer Analytics
  - Trends & Geography
- Connected directly to PostgreSQL warehouse

ğŸ“ Location:
dashboards/powerbi/

yaml
Copy code

---

## ğŸ” Key Architecture Principles

| Principle | Implementation |
|--------|----------------|
| Modularity | Separate scripts per pipeline stage |
| Idempotency | TRUNCATE + reload strategy |
| Fault Tolerance | Step-level error handling |
| Observability | Logs, reports, monitoring |
| Scalability | Layered schema design |
| BI-Readiness | Star schema warehouse |

---

## ğŸ§  Technology Stack

- **Language:** Python
- **Database:** PostgreSQL
- **Containerization:** Docker
- **Scheduling:** Python Scheduler
- **BI Tool:** Power BI
- **Testing:** Pytest + Coverage
- **Version Control:** Git & GitHub

---

## âœ… Conclusion
This architecture ensures a **robust, production-style data pipeline** that supports reliable analytics, automated execution, monitoring, and enterprise-ready BI reporting.

---
